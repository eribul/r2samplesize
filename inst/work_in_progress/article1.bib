@article{Efron1983,
abstract = {We construct a prediction rule on the basis of some data, and then wish to estimate the error rate of this rule in classifying future observations. Cross-validation provides a nearly unbiased estimate, using only the original data. Cross-validation turns out to be related ... $\backslash$n},
author = {Efron, Bradley},
doi = {10.1080/01621459.1983.10477973},
isbn = {01621459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {anova,article1,bootstrap,decomposition,logistic regression,prediction problem},
mendeley-tags = {article1},
number = {382},
pages = {316},
title = {{Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation}},
url = {http://www.jstor.org/stable/2288636?origin=crossref$\backslash$npapers3://publication/doi/10.2307/2288636},
volume = {78},
year = {1983}
}
@article{Efron1997,
abstract = {A study investigates the error rate of a rule for predicting future responses constructed from a training set of data. Results are nonparametric and apply to any possible prediction rule.},
annote = {Handlar om Bootstrap .632+ (vilket kanske {\"{a}}nnu inte {\"{a}}r helt relevant f{\"{o}}r oss).

Utv{\"{a}}rderar p{\aa} 24 olika modeller presenteade i tabell. Varierar sample size och dimensionality som vi. Dock stor skillnad att det baseras p{\aa} classification och inte regression. 

Anv{\"{a}}nder mindre samlpe sizes {\"{a}}n vi och fler variabler (st{\"{o}}rre p).

Har inte l{\"{a}}st s{\aa} noggrant d{\aa} den inte k{\"{a}}nns helt relevant och {\"{a}}r ganska teoretisk med mkt notation etc.},
author = {Efron, B. and Tibshirani, R.},
doi = {10.1080/01621459.1997.10474007},
file = {:C$\backslash$:/Users/eribu/Downloads/01621459{\%}2E1997{\%}2E10474007.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {article1,classification,cross-validation bootstrap,prediction rule},
mendeley-tags = {article1},
number = {438},
pages = {548},
pmid = {370},
title = {{Improvements on cross-validation: The .632 plus bootstrap method}},
volume = {92},
year = {1997}
}
@article{Nagelkerke1991,
abstract = {A generalization of the coefficient of determination R2 to general regression models is discussed. A modification of an earlier definition to allow for discrete models is proposed.},
author = {Nagelkerke, N. J D},
doi = {10.1093/biomet/78.3.691},
file = {:C$\backslash$:/Users/eribu/Downloads/nagelkerke{\_}n.j.d.{\_}1991{\_}-{\_}a{\_}note{\_}on{\_}a{\_}general{\_}definition{\_}of{\_}the{\_}coefficient{\_}of{\_}determination.pdf:pdf},
isbn = {0006-3444},
issn = {00063444},
journal = {Biometrika},
keywords = {Discrete probability,Log likelihood,Multiple correlation coefficient,Regression model,Residual variation,article1},
mendeley-tags = {article1},
number = {3},
pages = {691--692},
pmid = {339},
title = {{A note on a general definition of the coefficient of determination}},
volume = {78},
year = {1991}
}
@article{Jr1998,
author = {Jr, Frank E Harrell},
file = {:C$\backslash$:/Users/eribu/Downloads/logistic.val.pdf:pdf},
journal = {Statistics},
keywords = {article1},
mendeley-tags = {article1},
title = {{Comparison of Strategies for Validating Binary Logistic Regression Models}},
year = {1998}
}
@article{Fu2005,
abstract = {MOTIVATION: Estimation of misclassification error has received increasing attention in clinical diagnosis and bioinformatics studies, especially in small sample studies with microarray data. Current error estimation methods are not satisfactory because they either have large variability (such as leave-one-out cross-validation) or large bias (such as resubstitution and leave-one-out bootstrap). While small sample size remains one of the key features of costly clinical investigations or of microarray studies that have limited resources in funding, time and tissue materials, accurate and easy-to-implement error estimation methods for small samples are desirable and will be beneficial.$\backslash$n$\backslash$nRESULTS: A bootstrap cross-validation method is studied. It achieves accurate error estimation through a simple procedure with bootstrap resampling and only costs computer CPU time. Simulation studies and applications to microarray data demonstrate that it performs consistently better than its competitors. This method possesses several attractive properties: (1) it is implemented through a simple procedure; (2) it performs well for small samples with sample size, as small as 16; (3) it is not restricted to any particular classification rules and thus applies to many parametric or non-parametric methods.},
author = {Fu, Wenjiang J. and Carroll, Raymond J. and Wang, Suojin},
doi = {10.1093/bioinformatics/bti294},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu, Carroll, Wang - 2005 - Estimating misclassification error with small samples via bootstrap cross-validation.pdf:pdf},
isbn = {1367-4803 (Print)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
keywords = {article1},
mendeley-tags = {article1},
pmid = {15691862},
title = {{Estimating misclassification error with small samples via bootstrap cross-validation}},
year = {2005}
}
@article{Kim2009,
abstract = {We consider the accuracy estimation of a classifier constructed on a given training sample. The naive resubstitution estimate is known to have a downward bias problem. The traditional approach to tackling this bias problem is cross-validation. The bootstrap is another way to bring down the high variability of cross-validation. But a direct comparison of the two estimators, cross-validation and bootstrap, is not fair because the latter estimator requires much heavier computation. We performed an empirical study to compare the??.632+??bootstrap estimator with the repeated 10-fold cross-validation and the repeated one-third holdout estimator. All the estimators were set to require about the same amount of computation. In the simulation study, the repeated 10-fold cross-validation estimator was found to have better performance than the??.632+??bootstrap estimator when the classifier is highly adaptive to the training sample. We have also found that the??.632+??bootstrap estimator suffers from a bias problem for large samples as well as for small samples. ?? 2009 Elsevier B.V. All rights reserved.},
annote = {behandlar classification, inte regression.},
author = {Kim, Ji Hyun},
doi = {10.1016/j.csda.2009.04.009},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim - 2009 - Estimating classification error rate Repeated cross-validation, repeated hold-out and bootstrap.pdf:pdf},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {article1},
mendeley-tags = {article1},
number = {11},
pages = {3735--3745},
title = {{Estimating classification error rate: Repeated cross-validation, repeated hold-out and bootstrap}},
volume = {53},
year = {2009}
}
@article{Bioinformatics2006,
abstract = {Background: Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data. Results: We used CV to optimize the classification parameters for two kinds of classifiers; Shrunken Centroids and Support Vector Machines (SVM). Random training datasets were created, with no difference in the distribution of the features between the two classes. Using these "null" datasets, we selected classifier parameter values that minimized the CV error estimate. 10-fold CV was used for Shrunken Centroids while Leave-One-Out-CV (LOOCV) was used for the SVM. Independent test data was created to estimate the true error. With "null" and "non null" (with differential expression between the classes) data, we also tested a nested CV procedure, where an inner CV loop is used to perform the tuning of the parameters while an outer CV is used to compute an estimate of the error. The CV error estimate for the classifier with the optimal parameters was found to be a substantially biased estimate of the true error that the classifier would incur on independent data. Even though there is no real difference between the two classes for the "null" datasets, the CV error estimate for the Shrunken Centroid with the optimal parameters was less than 30{\%} on 18.5{\%} of simulated training data-sets. For SVM with optimal parameters the estimated error rate was less than 30{\%} on 38{\%} of "null" data-sets. Performance of the optimized classifiers on the independent test set was no better than chance. The nested CV procedure reduces the bias considerably and gives an estimate of the error that is very close to that obtained on the independent testing set for both Shrunken Centroids and SVM classifiers for "null" and "non-null" data distributions. Conclusion: We show that using CV to compute an error estimate for a classifier that has itself been tuned using CV gives a significantly biased estimate of the true error. Proper use of CV for estimating true error of a classifier developed using a well defined algorithm requires that all steps of the algorithm, including classifier parameter tuning, be repeated in each CV loop. A nested CV procedure provides an almost unbiased estimate of the true error.},
author = {Bioinformatics, Bmc and Varma, Sudhir and Simon, Richard},
doi = {10.1186/1471-2105-7-91},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bioinformatics, Varma, Simon - 2006 - Bias in error estimation when using cross-validation for model selection.pdf:pdf},
journal = {BMC Bioinformatics},
keywords = {article1},
mendeley-tags = {article1},
number = {7},
title = {{Bias in error estimation when using cross-validation for model selection}},
url = {http://www.biomedcentral.com/1471-2105/7/91},
volume = {7},
year = {2006}
}
@article{Isaksson2008,
abstract = {The interest in statistical classification for critical applications such as diagnoses of patient samples based on supervised learning is rapidly growing. To gain acceptance in applications where the subsequent decisions have serious consequences, e.g. choice of cancer therapy, any such decision support system must come with a reliable performance estimate. Tailored for small sample problems, cross-validation (CV) and bootstrapping (BTS) have been the most commonly used methods to determine such estimates in virtually all branches of science for the last 20 years. Here, we address the often overlooked fact that the uncertainty in a point estimate obtained with CV and BTS is unknown and quite large for small sample classification problems encountered in biomedical applications and elsewhere. To avoid this fundamental problem of employing CV and BTS, until improved alternatives have been established, we suggest that the final classification performance always should be reported in the form of a Bayesian confidence interval obtained from a simple holdout test or using some other method that yields conservative measures of the uncertainty. ?? 2008 Elsevier B.V. All rights reserved.},
author = {Isaksson, A. and Wallman, M. and G??ransson, H. and Gustafsson, M. G.},
doi = {10.1016/j.patrec.2008.06.018},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Isaksson et al. - 2008 - Cross-validation and bootstrapping are unreliable in small sample classification.pdf:pdf},
isbn = {01678655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Confidence interval,Performance estimation,Supervised classification,article1},
mendeley-tags = {article1},
title = {{Cross-validation and bootstrapping are unreliable in small sample classification}},
year = {2008}
}
@article{Steyerberg2001,
abstract = {The performance of a predictive model is overestimated when simply determined on the sample of subjects that was used to construct the model. Several internal validation methods are available that aim to provide a more accurate estimate of model performance in new subjects. We evaluated several variants of split-sample, cross-validation and bootstrapping methods with a logistic regression model that included eight predictors for 30-day mortality after an acute myocardial infarction. Random samples with a size between n = 572 and n = 9165 were drawn from a large data set (GUSTO-I; n = 40,830; 2851 deaths) to reflect modeling in data sets with between 5 and 80 events per variable. Independent performance was determined on the remaining subjects. Performance measures included discriminative ability, calibration and overall accuracy. We found that split-sample analyses gave overly pessimistic estimates of performance, with large variability. Cross-validation on 10{\%} of the sample had low bias and low variability, but was not suitable for all performance measures. Internal validity could best be estimated with bootstrapping, which provided stable estimates with low bias. We conclude that split-sample validation is inefficient, and recommend bootstrapping for estimation of internal validity of a predictive logistic regression model.},
annote = {Tipsad av Szilard.

fokuserad p{\aa} logistisk regression.

Utg{\aa}r fr{\aa}n praktiskt exempel med hj{\"{a}}rtattack. 
V{\aa}r ansats {\"{a}}r mer teoretisk/generell.
J{\"{a}}mf{\"{o}}r med olika EPV-v{\"{a}}rden och stratifiering.
Resamplar 500 ggr.
stepwise model selection med 8 predektorer.

Utv{\"{a}}rderar flera olika m{\"{a}}tv{\"{a}}rden, inkl R2 (dock Negelkerke).

Olika metoder men inte jack-knife.

F{\aa}r delvis samma slutsats som vi ang {\"{o}}verskattning med cv.},
author = {Steyerberg, Ewout W and Harrell, Frank E and Borsboom, Gerard J.J.M and Eijkemans, M.J.C and Vergouwe, Yvonne and Habbema, J.Dik F},
doi = {10.1016/S0895-4356(01)00341-9},
file = {:C$\backslash$:/Users/eribu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Steyerberg et al. - 2001 - Internal validation of predictive models.pdf:pdf},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Bootstrapping,Internal validation,Logistic regression analysis,Predictive models,article1,harrell,statistics},
mendeley-tags = {article1,harrell,statistics},
month = {aug},
number = {8},
pages = {774--781},
title = {{Internal validation of predictive models}},
url = {http://www.sciencedirect.com/science/article/pii/S0895435601003419},
volume = {54},
year = {2001}
}
